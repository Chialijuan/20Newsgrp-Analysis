{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import numpy as np\n",
    "from sklearn import datasets, model_selection\n",
    "import tarfile\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import spacy\n",
    "from string import digits\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting category names, i.e. folder names of 20 newsgroups folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\n",
      " 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\n",
      " 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\n",
      " 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\n",
      " 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\n",
      " 'talk.politics.misc' 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "path = \"20_newsgroups\"\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    newsgroups = np.array(dirs)\n",
    "print newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading files from first folder. Just use a for loop to read in from more folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(len(newsgroups)):\n",
    "    x = []\n",
    "    y=[]\n",
    "    path = \"20_newsgroups\" + \"/\" + newsgroups[i]\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            f = open(path + \"/\" + name)\n",
    "            x.append(f.read())\n",
    "            y.append(newsgroups[i])\n",
    "    X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
    "    x_train.extend(X_train)\n",
    "    x_test.extend(X_test)\n",
    "    y_train.extend(Y_train)\n",
    "    y_test.extend(Y_test)\n",
    "#print x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13997\n",
      "6000\n",
      "13997\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print len(x_train)\n",
    "print len(x_test)\n",
    "print len(y_train)\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each category \n",
      "comp.os.ms-windows.misc     700\n",
      "comp.windows.x              700\n",
      "rec.motorcycles             700\n",
      "sci.med                     700\n",
      "talk.politics.mideast       700\n",
      "rec.sport.baseball          700\n",
      "comp.graphics               700\n",
      "comp.sys.ibm.pc.hardware    700\n",
      "rec.autos                   700\n",
      "misc.forsale                700\n",
      "sci.electronics             700\n",
      "sci.space                   700\n",
      "talk.politics.guns          700\n",
      "talk.politics.misc          700\n",
      "rec.sport.hockey            700\n",
      "comp.sys.mac.hardware       700\n",
      "sci.crypt                   700\n",
      "talk.religion.misc          700\n",
      "alt.atheism                 700\n",
      "soc.religion.christian      697\n",
      "Name: y_train, dtype: int64.\n",
      "Counts for each category \n",
      "talk.politics.guns          300\n",
      "sci.space                   300\n",
      "rec.motorcycles             300\n",
      "sci.med                     300\n",
      "talk.politics.misc          300\n",
      "talk.politics.mideast       300\n",
      "rec.sport.hockey            300\n",
      "comp.sys.mac.hardware       300\n",
      "rec.sport.baseball          300\n",
      "comp.graphics               300\n",
      "sci.crypt                   300\n",
      "talk.religion.misc          300\n",
      "comp.sys.ibm.pc.hardware    300\n",
      "alt.atheism                 300\n",
      "comp.windows.x              300\n",
      "rec.autos                   300\n",
      "misc.forsale                300\n",
      "comp.os.ms-windows.misc     300\n",
      "sci.electronics             300\n",
      "soc.religion.christian      300\n",
      "Name: y_test, dtype: int64.\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.DataFrame(y_train, columns = ['y_train'])\n",
    "print 'Counts for each category \\n%s.' % y_train.y_train.value_counts()\n",
    "\n",
    "y_test = pd.DataFrame(y_test, columns = ['y_test'])\n",
    "\n",
    "print 'Counts for each category \\n%s.' % y_test.y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain metadatas From, Subject, Date, Organization, Sender from the headers of each file. Obtain Content from the body of each file and place in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the dataframe and assigning columns\n",
    "def create_dataframe(x): \n",
    "    original = pd.DataFrame()\n",
    "    original = original.assign(Path = '')\n",
    "    original = original.assign(From = '')\n",
    "    original = original.assign(Subject = '')\n",
    "    original = original.assign(Date = '')\n",
    "    original = original.assign(Organization = '')\n",
    "    original = original.assign(Sender = '')\n",
    "    original = original.assign(Content = '')\n",
    "    original = original.assign(Lines = '')\n",
    "    original = original.assign(Nntp_posting_host = '')\n",
    "    original = original.assign(Message_id = '')\n",
    "    original = original.assign(Reply_to = '')\n",
    "    original = original.assign(References = '')\n",
    "    #print original.dtypes\n",
    "\n",
    "    # Placing the metadatas and content in the respective cells of the dataframe\n",
    "    for i in range(len(x)):\n",
    "        data = x[i].split(\"\\n\\n\", 1)\n",
    "        email_content = ''\n",
    "        meta = sorted(data[0].split(\"\\n\"))\n",
    "        for k in meta:\n",
    "            z = np.array(k.split(\":\", 1))\n",
    "            meta_title = z[0].capitalize().replace('-', '_')\n",
    "            if meta_title in list(original):\n",
    "                try:\n",
    "                    original.at[i, meta_title] = z[1]\n",
    "                except ValueError:\n",
    "                    original.at[i, z[0]] = np.NaN\n",
    "        original.at[i, 'Content'] = data[1]\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path                 13997\n",
       "From                 13997\n",
       "Subject              13997\n",
       "Date                 13997\n",
       "Organization         13415\n",
       "Sender                7618\n",
       "Content              13997\n",
       "Lines                13953\n",
       "Nntp_posting_host     5974\n",
       "Message_id           13997\n",
       "Reply_to              2102\n",
       "References            8750\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = create_dataframe(x_train)\n",
    "train_x.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path                 6000\n",
       "From                 6000\n",
       "Subject              6000\n",
       "Date                 6000\n",
       "Organization         5731\n",
       "Sender               3254\n",
       "Content              6000\n",
       "Lines                5984\n",
       "Nntp_posting_host    2601\n",
       "Message_id           6000\n",
       "Reply_to              923\n",
       "References           3758\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = create_dataframe(x_test)\n",
    "test_x.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.to_csv('train_x.csv')\n",
    "test_x.to_csv('test_x.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emails\n",
    "Extract email address from From and Sender column. From and Sender names were not considered as not much info can be gotten. \n",
    "Flag whether the document is from an educational institution by checking the domain for .edu\n",
    "Extra: extract domain, country etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_emails(original):\n",
    "    # Extracting the email addresses from From column\n",
    "    original['Email_from'] = original.From.map(lambda x : re.search(r'[\\w\\.-]+@[\\w\\.-]+', x).group(0) \n",
    "                                               if re.search(r'[\\w\\.-]+@[\\w\\.-]+', x) is not None \n",
    "                                               else np.NaN)\n",
    "\n",
    "    # Extracting the email addresses from Sender column\n",
    "    original['Email_sender'] = original.Sender.map(lambda x : np.NaN \n",
    "                                                   if pd.isnull(x) \n",
    "                                                   else re.search(r'[\\w\\.-]+@[\\w\\.-]+', x).group(0) \n",
    "                                                   if re.search(r'[\\w\\.-]+@[\\w\\.-]+', x) is not None \n",
    "                                                   else np.NaN)\n",
    "\n",
    "    # flag as educational organisation if the email address has .edu domain\n",
    "    original['Edu_org'] = original.Email_from.map(lambda x : 1 if isinstance(x, basestring) and x[len(x) - 3:].find('edu') is not -1 \n",
    "                                                  else 0)\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = process_emails(train_x)\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject\n",
    "For Subject, remove Re: (maybe indicate that its a reply message), throw to content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_subject(original):\n",
    "    # Process the subject to remove Re:\n",
    "    original['Subject_processed'] = original.Subject.map(lambda x : x[x.find(\"FAQ:\") + 5:] \n",
    "                                                         if x.find(\"FAQ:\") is not -1 \n",
    "                                                         else x[x.find(\"Re:\") + 4:] if x.find(\"Re:\") is not -1 and x.find(\"Re\", 5) is -1\n",
    "                                                         else x[x.find(\"Re:\") + 4:x.find(\"Re\", 5)] if x.find(\"Re:\") is not -1\n",
    "                                                         else x)\n",
    "\n",
    "    # Reply_msg = 1 if the document is a reply message, otherwise 0\n",
    "    original['Reply_msg'] = original.Subject.map(lambda x : 1 \n",
    "                                                 if x.find(\"Re:\") == 1 \n",
    "                                                 else 0)\n",
    "\n",
    "    # Adds the processed subject to Content column\n",
    "    original['Content'] = original.Content + \" \" + original.Subject_processed\n",
    "\n",
    "    # Removes Subject_processed column as they are appended to content column\n",
    "    del original['Subject_processed']\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = process_subject(train_x)\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date\n",
    "For Date, extract month and year (year seems to be all in 1993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_date(original):\n",
    "    # Extracting the Month from the Date column\n",
    "    month_dict = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "    original['Month'] = original.Date.map(lambda y : ''.join(filter(lambda x: x.lower() in month_dict, y.split())))\n",
    "\n",
    "    # Extracting the Year from the Date column\n",
    "    original['Year'] = original.Date.map(lambda x : ''.join([x[x.lower().find(i):].split(' ')[1] \n",
    "                                                             for i in month_dict \n",
    "                                                             if x.lower().find(i) is not -1]))\n",
    "    original['Year'] = original.Year.map(lambda x : x[2:] \n",
    "                                         if len(x) is 4 \n",
    "                                         else x)\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = process_date(train_x)\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the columns of original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows that some columns have missing values, such as Organisation, Sender, Lines, Email_from, and Email_sender (might need to remove email_from and email_sender and subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, when using full dataset, if all years are in 93, remove the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.groupby('Year').count()['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking, all years (except two cases) are in 1993 so year column is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x['Year']\n",
    "print list(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Month Column, we see that there is one entry which is empty. Further checks shows that it is because the month is spelt as April instead of the usual Apr, therefore the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.groupby('Month').count()['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp = ''\n",
    "for i in range(len(train_x.Month)):\n",
    "    if train_x.Month[i] == '':\n",
    "        emp = i\n",
    "print train_x.Date[emp]\n",
    "train_x.at[emp, 'Month'] = 'Apr'\n",
    "train_x.groupby('Month').count()['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Educational Organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print train_x.groupby('Edu_org').count()['Content']\n",
    "ax = seaborn.countplot(x='Edu_org', data=train_x)\n",
    "ax.set(title = 'Count of Educational Organisations', xlabel = 'Educational Organisation', xticklabels = [\"Yes\", 'No'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print train_x.groupby('Reply_msg').count()['Content']\n",
    "ax = seaborn.countplot(x='Reply_msg', data=train_x)\n",
    "ax.set(title = 'Count of Documents that are replies', xlabel = 'Reply Documents', xticklabels = [\"Yes\", 'No'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.groupby('Organization').count()['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(document):\n",
    "    # Removes emails\n",
    "    document = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', document)\n",
    "    \n",
    "    # Removes URLs\n",
    "    document = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', document)\n",
    "    \n",
    "    # Removes punctuations\n",
    "    document = document.translate(None, string.punctuation)\n",
    "    \n",
    "    # Change to lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Removes newline characters\n",
    "    document = ' '.join(document.split('\\n'))\n",
    "    \n",
    "    # Removes ASCII\n",
    "    document = re.sub(r'[^\\x00-\\x7F]+',' ', document)\n",
    "    \n",
    "    # Removes numbers\n",
    "    document = document.translate(None, digits)\n",
    "    \n",
    "    # Replace multiple spaces with one space\n",
    "    document = ' '.join(document.split())\n",
    "    #document = nlp(unicode(document))\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_tokenizer(document):\n",
    "    parser = English()\n",
    "    document = parser(unicode(document))\n",
    "    # Removes stopwords\n",
    "    document = [tok for tok in document if (str(tok) not in stopwords)]\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_content = train_x.Content.map(lambda x : clean_text(x))\n",
    "train_x_content = train_x_content.map(lambda x : spacy_tokenizer(x))\n",
    "test_x_content = test_x.Content.map(lambda x : clean_text(x))\n",
    "test_x_content = test_x_content.map(lambda x : spacy_tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "Transforming into DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "linear_svc = LinearSVC()\n",
    "pipe = Pipeline([('count_vect', count_vect), ('linear_svc', linear_svc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "train_x_stem = train_x_content.map(lambda x : ' '.join([porter_stemmer.stem(str(i)) for i in x]))\n",
    "test_x_stem = test_x_content.map(lambda x : ' '.join([porter_stemmer.stem(str(i)) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.fit([x for x in train_x_stem], [y for y in y_train])\n",
    "pred = pipe.predict(test_x_stem)\n",
    "print \"Accuracy:\", accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "train_x_lem = train_x_content.map(lambda x : ' '.join([wordnet_lemmatizer.lemmatize(str(i)) for i in x]))\n",
    "test_x_lem = test_x_content.map(lambda x : ' '.join([wordnet_lemmatizer.lemmatize(str(i)) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7805\n"
     ]
    }
   ],
   "source": [
    "pipe.fit([x for x in train_x_lem], [y for y in y_train])\n",
    "pred = pipe.predict(test_x_lem)\n",
    "print \"Accuracy:\", accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.780666666667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
