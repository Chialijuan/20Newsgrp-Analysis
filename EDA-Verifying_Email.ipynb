{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from email.parser import Parser\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 19997\n",
      "                                                    Body   Category\n",
      "19992  Xref: cantaloupe.srv.cs.cmu.edu alt.sci.planet...  sci.space\n",
      "19993  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...  sci.space\n",
      "19994  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...  sci.space\n",
      "19995  Xref: cantaloupe.srv.cs.cmu.edu sci.space:6078...  sci.space\n",
      "19996  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...  sci.space\n"
     ]
    }
   ],
   "source": [
    "# fpath = '20_newsgroups/alt.athesim_comp'\n",
    "fpath = '20_newsgroups'\n",
    "messagefolder =[]\n",
    "\n",
    "for root, dirs, files in os.walk(fpath, topdown=False):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        messagecat = os.path.basename(os.path.dirname(filename))\n",
    "\n",
    "        with codecs.open(filename, 'r', encoding='utf_8', errors='ignore') as f:\n",
    "            messagefolder.append({\"Category\":messagecat, \"Body\":f.read()})\n",
    "        \n",
    "            \n",
    "messagefolder = pd.DataFrame(messagefolder)\n",
    "print(\"Number of documents: {}\".format(len(messagefolder)))\n",
    "print(messagefolder.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def message_parser(messagefile, cat):\n",
    "    headers = Parser().parsestr(messagefile,headersonly=True)\n",
    "\n",
    "    # Get email's body\n",
    "    if headers.is_multipart():\n",
    "        for part in headers.get_payload():\n",
    "            txt = part.get_payload()\n",
    "    else:\n",
    "        txt = headers.get_payload()\n",
    "        \n",
    "    return {\"Category\": cat,\n",
    "                       'Path':headers['Path'],\n",
    "                       'From': headers['From'],\n",
    "                      'Newsgroups': headers['Newsgroups'],\n",
    "                      'Subject': headers['Subject'],\n",
    "                      'Date': headers['Date'],\n",
    "                      'Organization':headers['Organization'],\n",
    "                      'NNTP-Posting-Host':headers['NNTP-Posting-Host'],\n",
    "                      'References': headers['References'],\n",
    "                      'Message-ID': headers['Message-ID'],\n",
    "                      'Sender': headers['Sender'],\n",
    "                      'Body': txt.replace('\\n', '').strip(),\n",
    "                      'Lines': headers['Lines']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_full = messagefolder.apply(lambda x: pd.Series(message_parser(x['Body'],x['Category'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = msg_full.ix[:,msg_full.columns != 'Category']\n",
    "y = msg_full['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "\n",
    "nlp = spacy.load('en')\n",
    "punc = string.punctuation\n",
    "\n",
    "stopw = [x for x in stopwords]\n",
    "stopw.extend(['what','when','who','why', 'X', 'article', 'thing', 'way', '-PRON-'])\n",
    "\n",
    "def clean_text(msg):\n",
    "    try:\n",
    "        # Removes emails\n",
    "        msg = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', msg)\n",
    "        \n",
    "        # Removes URLs\n",
    "        msg = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', msg)\n",
    "        \n",
    "        # Removes ASCII\n",
    "        msg = re.sub(r'[^\\x00-\\x7F]+',' ', msg)\n",
    "        \n",
    "        # Removes newline characters\n",
    "        msg = ' '.join(msg.split('\\n'))\n",
    "        \n",
    "        # Convert into spacy token\n",
    "        msg = nlp(msg)\n",
    "        \n",
    "        #Lemmatize, remove stopwords and punctuations\n",
    "        tokens = [str(token.lemma_) for token in msg]\n",
    "        tokens = [tok for tok in tokens if (tok not in stopw and tok not in punc)] \n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    except:\n",
    "        print('error')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = msg.assign(clean_body = msg['Body'].apply(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'misc.entrepreneurs,misc.wanted,pnw.forsale,uw.pc.ibm,seattle.forsale,uw..forsale,misc.forsale,misc.forsale.computers.d,misc.forsale.computers.pc-clone,misc.forsale.coomputers.other,Distribution: worldFollowup-To: From:yuri@atmos.washington.eduReply-To: yuri@atmos.washington.eduOrganization: Subject: 100 simms and 100 sipps  1MB neededKeywords: \\t\\tI need  100 simms and 100 sipps 1MB, but price should be around $17-20/piece.I am waiting for an offer.\\tYuri Yulaev\\t6553, 38th ave NE\\tSeattle WA 98115\\t(206) 524-2806,524-9547 (home)\\t(206) 685-3793 (work)\\t(206) 524-7218 (FAX)INTERNET: yuri@atmos.washington.eduUUCP:\\t  uw-beaver!atmos.washington.edu!yuri'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg['Body'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'misc.entrepreneurs,misc.wanted,pnw.forsale,uw.pc.ibm,seattle.forsale,uw..forsale,misc.forsale,misc.forsale.computers.d,misc.forsale.computers.pc-clone,misc.forsale.coomputers.other,distribution worldfollowup   subject 100 simms 100 sipp   1 mb neededkeyword \\t\\t ne   100 simms 100 sipp 1 mb price 17 20/piece wait offer \\t yuri yulaev \\t 6553 38th ave ne \\t seattle wa 98115 \\t 206 524 2806,524 9547 home \\t 206 685 3793 work \\t 206 524 7218 fax)internet   \\t   uw beaver!atmos.washington.edu!yuri'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(msg['Body'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Categorize emails into real and fake senders based on the mapping of `Path` and `From`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering on Path and From\n",
    "msg = msg.assign(path_token = msg['Path'].apply(lambda x: x.split('!')[-1]))\n",
    "msg = msg.assign(sender = msg['From'].apply(lambda x: x.split('@')[0]))\n",
    "\n",
    "# create new column that indicates if email is legit\n",
    "msg['true_email'] = np.where((msg['path_token'] == msg['sender']), 1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of emails: 14997\n",
      "No. of emails with legit senders: 11669\n",
      "No. of emails with false senders: 3328\n"
     ]
    }
   ],
   "source": [
    "print('Total no. of emails: {}'.format(len(train)))\n",
    "print('No. of emails with legit senders: {}'.format(sum(train['true_email'])))\n",
    "print('No. of emails with false senders: {}'.format(len(train)-sum(train['true_email'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(msg, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Union\n",
    "- Combining features of different data types into a single feature matrix using `FeatureUnion` allows us to do gridsearch easily.\n",
    "- Using `FunctionTransformer`, `FeatureUnion`, `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_true_email(df):\n",
    "    return df.loc[:, ['true_email','Lines']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14997, 2)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a stateless transformer from get_true_email function\n",
    "get_true_email_ft = FunctionTransformer(get_true_email, validate=False)\n",
    "get_true_email_ft.transform(x_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(df):\n",
    "    return df['clean_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14997,)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create stateless transformer from get_text function\n",
    "get_text_ft = FunctionTransformer(get_text, validate=False)\n",
    "get_text_ft.transform(x_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for gridsearch of nested Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('featureunion', FeatureUnion([\n",
    "    ('pipeline', Pipeline([\n",
    "        ('functiontransformer', get_text_ft),\n",
    "        ('tfidfvectorizer', vect)])),\n",
    "     ('functiontransformer', get_true_email_ft)\n",
    "])),\n",
    "    ('multinomialnb', nb)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid ={\n",
    "    'featureunion__pipeline__tfidfvectorizer__token_pattern':[r\"\\b\\w\\w+\\b\", r\"'([a-z ]+)'\"],\n",
    "    'featureunion__pipeline__tfidfvectorizer__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "    'featureunion__pipeline__tfidfvectorizer__norm':['l1','l2'],\n",
    "    'multinomialnb__alpha' : [0.1,0.5, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14997,)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14997, 16)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[(u'featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[(u'pipeline', Pipeline(steps=[(u'functiontransformer', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function get_text at 0x7fe1a7195aa0>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y=False, va...rmer_weights=None)), (u'multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={u'featureunion__pipeline__tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)], u'multinomialnb__alpha': [0.1, 0.5, 1], u'featureunion__pipeline__tfidfvectorizer__token_pattern': [u'\\\\b\\\\w\\\\w+\\\\b', u\"'([a-z ]+)'\"], u'featureunion__pipeline__tfidfvectorizer__norm': [u'l1', u'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=u'accuracy', verbose=0)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865973194639\n",
      "{u'featureunion__pipeline__tfidfvectorizer__ngram_range': (1, 2), u'multinomialnb__alpha': 0.1, u'featureunion__pipeline__tfidfvectorizer__token_pattern': u'\\\\b\\\\w\\\\w+\\\\b', u'featureunion__pipeline__tfidfvectorizer__norm': u'l2'}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
