{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 650 Ti (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.similarities.docsim import Similarity\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk.data\n",
    "import string\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim import matutils\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iteminfo_train = pd.read_csv('ItemInfo_train.csv')\n",
    "#iteminfo_test = pd.read_csv('ItemInfo_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itempairs_train = pd.read_csv('ItemPairs_train.csv')\n",
    "#itempairs_test = pd.read_csv('ItemPairs_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = pd.read_csv('Location.csv')\n",
    "category = pd.read_csv('Category.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iteminfo_train_loc = pd.merge(pd.merge(iteminfo_train, location, how = 'left', on = 'locationID'), category, how = 'left', on = 'categoryID')\n",
    "#iteminfo_test_loc = pd.merge(pd.merge(iteminfo_test, location, how = 'left', on = 'locationID'), category, how = 'left', on = 'categoryID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pd.merge(pd.merge(itempairs_train, iteminfo_train_loc, how = 'left', left_on = 'itemID_1', right_on = 'itemID'), \n",
    "                    iteminfo_train_loc, how = 'left', left_on = 'itemID_2', right_on = 'itemID')\n",
    "#x_test = pd.merge(pd.merge(itempairs_test, iteminfo_test_loc, how = 'left', left_on = 'itemID_1', right_on = 'itemID'), \n",
    "#                    iteminfo_test_loc, how = 'left', left_on = 'itemID_2', right_on = 'itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x['title_x'] = x.title_x.map(lambda x : '' if pd.isnull(x) else x)\n",
    "x['title_y'] = x.title_y.map(lambda x : '' if pd.isnull(x) else x)\n",
    "x['description_x'] = x.description_x.map(lambda x : '' if pd.isnull(x) else x)\n",
    "x['description_y'] = x.description_y.map(lambda x : '' if pd.isnull(x) else x)\n",
    "#x_test['title_x'] = x_test.title_x.map(lambda x : '' if pd.isnull(x) else x)\n",
    "#x_test['title_y'] = x_test.title_y.map(lambda x : '' if pd.isnull(x) else x)\n",
    "#x_test['description_x'] = x_test.description_x.map(lambda x : '' if pd.isnull(x) else x)\n",
    "#x_test['description_y'] = x_test.description_y.map(lambda x : '' if pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(document):    \n",
    "    # Removes punctuations\n",
    "    document = document.translate(None, string.punctuation)\n",
    "    \n",
    "    # Change to lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Removes newline characters\n",
    "    document = ' '.join(document.split('\\n'))\n",
    "    \n",
    "    # Replace multiple spaces with one space\n",
    "    document = ' '.join(document.split())\n",
    "    #document = nlp(unicode(document))\n",
    "    \n",
    "    return document\n",
    "\n",
    "def word_vec(x, model, num_features):\n",
    "    try:\n",
    "        a = model[x]\n",
    "        b = 1\n",
    "    except KeyError:\n",
    "        a = np.zeros(num_features)\n",
    "        b = 0\n",
    "    return [a,b]\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    \n",
    "    for word in words:\n",
    "        a, b = word_vec(word, model, num_features)\n",
    "        nwords = nwords + b\n",
    "        featureVec = np.add(featureVec, a)\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords != 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "10\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "sentences_desc_x = []\n",
    "sentences_desc_y = []\n",
    "sentences_title_x = []\n",
    "sentences_title_y = []\n",
    "for i in range(len(x.description_x)):\n",
    "    if i in [10, 500000, 1000000, 1500000, 2000000]:\n",
    "        print i\n",
    "    sentences_desc_x.append(clean_text(x.description_x[i]).decode('utf-8').split())\n",
    "    sentences_desc_y.append(clean_text(x.description_y[i]).decode('utf-8').split())\n",
    "for i in range(len(x.title_x)):\n",
    "    if i in [10, 500000, 1000000, 1500000, 2000000]:\n",
    "        print i\n",
    "    sentences_title_x.append(clean_text(x.title_x[i]).decode('utf-8').split())\n",
    "    sentences_title_y.append(clean_text(x.title_y[i]).decode('utf-8').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_x\n",
      "w2v_y\n",
      "w2v_x\n",
      "w2v_y\n",
      "cos_sim_x\n",
      "Test Accuracy: 0.608407434646\n",
      "Train Accuracy: 0.60927426836\n",
      "w2v_x\n",
      "w2v_y\n",
      "w2v_x\n",
      "w2v_y\n",
      "cos_sim_x\n",
      "Test Accuracy: 0.601832924104\n",
      "Train Accuracy: 0.601615800648\n",
      "w2v_x\n",
      "w2v_y\n",
      "w2v_x\n",
      "w2v_y\n",
      "cos_sim_x\n",
      "Test Accuracy: 0.607101369094\n",
      "Train Accuracy: 0.607410753423\n",
      "w2v_x\n",
      "w2v_y\n",
      "w2v_x\n",
      "w2v_y\n",
      "cos_sim_x\n",
      "Test Accuracy: 0.603813605358\n",
      "Train Accuracy: 0.605709206863\n",
      "w2v_x\n",
      "w2v_y\n",
      "w2v_x\n",
      "w2v_y\n",
      "cos_sim_x\n",
      "Test Accuracy: 0.610148442449\n",
      "Train Accuracy: 0.611352474618\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, random_state = 1)\n",
    "train_score = []\n",
    "test_score = []\n",
    "log_reg = LogisticRegression()\n",
    "for train_index, test_index in kf.split(x):\n",
    "    sentences_desc_x_train = np.array(sentences_desc_x)[train_index].tolist()\n",
    "    sentences_desc_y_train = np.array(sentences_desc_y)[train_index].tolist()\n",
    "    sentences_desc_x_test = np.array(sentences_desc_x)[test_index].tolist()\n",
    "    sentences_desc_y_test = np.array(sentences_desc_y)[test_index].tolist()\n",
    "    sentences_title_x_train = np.array(sentences_title_x)[train_index].tolist()\n",
    "    sentences_title_y_train = np.array(sentences_title_y)[train_index].tolist()\n",
    "    sentences_title_x_test = np.array(sentences_title_x)[test_index].tolist()\n",
    "    sentences_title_y_test = np.array(sentences_title_y)[test_index].tolist()\n",
    "    print 'w2v_x'\n",
    "    model_desc_x = Word2Vec(sentences_desc_x_train, min_count = 1, size = 50)\n",
    "    print 'w2v_y'\n",
    "    model_desc_y = Word2Vec(sentences_desc_y_train, min_count = 1, size = 50)\n",
    "    print 'w2v_x'\n",
    "    model_title_x = Word2Vec(sentences_title_x_train, min_count = 1, size = 50)\n",
    "    print 'w2v_y'\n",
    "    model_title_y = Word2Vec(sentences_title_y_train, min_count = 1, size = 50)\n",
    "    print 'cos_sim_x'\n",
    "    x_train, x_test = x.loc[train_index, :], x.loc[test_index, :]\n",
    "    cos_sim_desc_train = []\n",
    "    cos_sim_desc_test = []\n",
    "    num_features = model_desc_x.wv.syn0.shape[1]\n",
    "    for i in range(len(sentences_desc_x_train)):\n",
    "        cos_sim_desc_train.append(cosine_similarity(np.array([makeFeatureVec(sentences_desc_x_train[i], model_desc_x, num_features),\n",
    "                                                        makeFeatureVec(sentences_desc_y_train[i], model_desc_y, num_features)]))[1, 0])\n",
    "\n",
    "    for i in range(len(sentences_desc_x_test)):\n",
    "        cos_sim_desc_test.append(cosine_similarity(np.array([makeFeatureVec(sentences_desc_x_test[i], model_desc_x, num_features),\n",
    "                                                        makeFeatureVec(sentences_desc_y_test[i], model_desc_y, num_features)]))[1, 0])\n",
    "    \n",
    "    cos_sim_title_train = []\n",
    "    cos_sim_title_test = []\n",
    "    num_features = model_desc_x.wv.syn0.shape[1]\n",
    "    for i in range(len(sentences_title_x_train)):\n",
    "        cos_sim_title_train.append(cosine_similarity(np.array([makeFeatureVec(sentences_title_x_train[i], model_title_x, num_features),\n",
    "                                                        makeFeatureVec(sentences_title_y_train[i], model_title_y, num_features)]))[1, 0])\n",
    "\n",
    "    for i in range(len(sentences_desc_x_test)):\n",
    "        cos_sim_title_test.append(cosine_similarity(np.array([makeFeatureVec(sentences_title_x_test[i], model_title_x, num_features),\n",
    "                                                        makeFeatureVec(sentences_title_y_test[i], model_title_y, num_features)]))[1, 0])\n",
    "    \n",
    "    log_reg.fit(pd.DataFrame(np.column_stack([cos_sim_desc_train, cos_sim_title_train])), np.array(x_train.isDuplicate))\n",
    "    pred_train = log_reg.predict(pd.DataFrame(np.column_stack([cos_sim_desc_train, cos_sim_title_train])))\n",
    "    pred_test = log_reg.predict(pd.DataFrame(np.column_stack([cos_sim_desc_test, cos_sim_title_test])))\n",
    "    train_score.append(pred_train)\n",
    "    test_score.append(pred_test)\n",
    "    print \"Test Accuracy:\", accuracy_score(x_test.isDuplicate, pred_test)\n",
    "    print \"Train Accuracy:\", accuracy_score(x_train.isDuplicate, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
